{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, imageio\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist= datasets.MNIST(root='./../data', train=True,download=False, transform=mnist_trans)\n",
    "mnist = torch.utils.data.random_split(mnist, [10000, len(mnist)-10000])[0] # gets split into two parts [10k and 50k] and we slice 0 index.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.0.dev20230622+cu121  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pixel_coordinates(img):\n",
    "    \n",
    "    height, width, _ = img.permute(1,2,0).shape\n",
    "    \n",
    "    # Generate coordinates along the x-axis and y-axis\n",
    "    x_coords = np.linspace(0, 1, width, endpoint=False)\n",
    "    y_coords = np.linspace(0, 1, height, endpoint=False)\n",
    "    \n",
    "    # Create a meshgrid of coordinates\n",
    "    x_mesh, y_mesh = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "    # Stack the coordinates and reshape to obtain the final output\n",
    "    coordinates = np.stack([x_mesh, y_mesh], axis=-1)\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Feature mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_mapping(x, B):\n",
    "    '''\n",
    "        cos and sin of input are joined together to increased the dimension to 4.\n",
    "    '''\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        B = B.to(x)\n",
    "        x_proj = (2.*np.pi*x) @ B.T #512,2\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)    #  2*len(B) #512,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(coord_input,image,map_dict,mapping = 'none'):\n",
    "\n",
    "    coord_input = input_mapping(coord_input, map_dict[mapping])\n",
    "    \n",
    "    test_data = [coord_input, image]\n",
    "    train_data = [coord_input[::2], image[::2]]\n",
    "    \n",
    "    train_x = torch.tensor(train_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    train_y = torch.tensor(train_data[1])#.reshape(-1,3)\n",
    "    test_x = torch.tensor(test_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    test_y = torch.tensor(test_data[1])#.reshape(-1,3)\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50) # input has 4 dimensions.\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,4) # input has 4 dimensions.\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x,train_y,test_x,test_y = batch_loader(xy_grid,x.permute(1,2,0),B_dict,keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = test_x.to(device)\n",
    "# target = test_y.to(device)\n",
    "\n",
    "# model = Net(input.shape[2]).to(device)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(1501)):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     generated = model(input)\n",
    "\n",
    "#     loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#       print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "#       plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "#       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(coordinates,image,img_no,dictionary,mapping,save = False):\n",
    "\n",
    "    train_x,train_y,test_x,test_y = batch_loader(coordinates,image.permute(1,2,0),dictionary,mapping)\n",
    "\n",
    "    input = test_x.to(device)\n",
    "    target = test_y.to(device)\n",
    "\n",
    "    model = Net(input.shape[2]).to(device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "\n",
    "    for epoch in range(1501):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        generated = model(input)\n",
    "\n",
    "        loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 300 == 0:\n",
    "        #     print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "        #     plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        #     plt.show()\n",
    "\n",
    "    # if save:\n",
    "    #     torch.save(model.state_dict(), '../weights/{fname}.pth'.format(fname=img_no))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_size = 64\n",
    "\n",
    "B_dict = {}\n",
    "\n",
    "B_gauss = torch.normal(0,1,size=(mapping_size,2))\n",
    "for scale in [10.]:\n",
    "    B_dict[f'gauss_{scale}'] = B_gauss * scale\n",
    "\n",
    "for k in B_dict:\n",
    "    keys = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = mnist[0]\n",
    "\n",
    "xy_grid = torch.from_numpy(generate_pixel_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0329747200012207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d778456cd63247b5a94b35989a6322b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MLAI\\lib\\site-packages\\torch\\utils\\_device.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm(range(len(mnist))):\n",
    "\n",
    "    model_weights(xy_grid,mnist[index][0],index,B_dict,keys,save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0795,  0.1682,  0.0875,  ..., -0.2525, -0.0463,  0.0028],\n",
      "        [ 0.0552, -0.0240,  0.0436,  ..., -0.0965, -0.0113,  0.1088],\n",
      "        [ 0.1314, -0.1306,  0.1767,  ..., -0.0286, -0.2119,  0.1687],\n",
      "        ...,\n",
      "        [-0.0419,  0.0082,  0.2348,  ..., -0.3616, -0.2333, -0.0336],\n",
      "        [-0.0057,  0.0810, -0.1398,  ...,  0.1748,  0.3189, -0.1856],\n",
      "        [ 0.0761,  0.0039,  0.0316,  ...,  0.1862,  0.0608,  0.1784]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0222, -0.2944, -0.0153, -0.1830, -0.1238, -0.7707, -0.4240,  0.0094,\n",
      "        -0.4637, -0.4999, -0.0846, -0.8748,  0.2038, -0.7559,  0.0421,  0.3459,\n",
      "        -0.3898, -0.9280,  0.0213, -0.9460, -0.0888, -0.0765, -0.0522,  0.0309,\n",
      "        -0.7213, -0.4241, -0.1429, -0.5051,  0.0449, -0.2816, -0.7010, -0.0519,\n",
      "        -0.2336, -0.1436, -0.0351, -0.7080,  0.2282, -0.4984,  0.1681, -0.6942,\n",
      "        -0.1610, -0.4047, -0.6369, -0.4612, -0.3822, -0.5614, -0.1953, -0.1232,\n",
      "        -0.1155, -0.6370], device='cuda:0')\n",
      "tensor([[-3.4537e-03, -3.1526e-01, -2.3091e-01,  ..., -2.1225e-01,\n",
      "          4.3251e-02,  5.2750e-02],\n",
      "        [-4.2888e-01, -2.9787e-01, -1.4177e-01,  ..., -1.4610e-01,\n",
      "         -5.6046e-01,  7.8951e-02],\n",
      "        [ 2.6668e-02,  4.0909e-02, -1.6418e-01,  ..., -1.3427e-01,\n",
      "         -6.8873e-05,  1.6477e-02],\n",
      "        ...,\n",
      "        [-6.2107e-02, -1.5601e-01, -1.2857e-01,  ..., -1.7867e-01,\n",
      "         -1.5721e-01, -1.9353e-01],\n",
      "        [-1.7935e-01, -3.9825e-02, -3.3364e-01,  ..., -4.0625e-01,\n",
      "         -2.2289e-01, -6.7341e-04],\n",
      "        [-8.4336e-02,  2.7919e-02, -2.4141e-01,  ..., -2.6219e-01,\n",
      "          3.0566e-02, -2.0151e-02]], device='cuda:0')\n",
      "tensor([ 0.6000,  0.4661,  0.3476, -0.0875,  0.3630,  0.0178,  0.0685,  0.5175,\n",
      "         0.0709,  0.2027, -0.0639, -0.1280,  0.3550,  0.7378,  0.8143,  0.6735,\n",
      "         0.5465,  0.6706,  0.4014,  0.1077,  0.1005, -0.0532,  0.5804,  0.6775,\n",
      "         0.3031,  0.1113,  0.0546,  0.6098,  0.2375,  0.3189, -0.0387,  0.0062,\n",
      "         0.2024,  0.5116, -0.0314,  0.2890, -0.0080, -0.0697,  0.5280,  0.8387,\n",
      "         0.6422, -0.1137,  0.5034,  0.4685,  0.2571, -0.1925,  0.4661, -0.1226,\n",
      "         1.0742,  0.8327], device='cuda:0')\n",
      "tensor([[ 0.0305,  0.0370,  0.0607,  0.0088,  0.0441,  0.0478,  0.0238,  0.0549,\n",
      "          0.0497,  0.0436,  0.0134,  0.0295,  0.0702,  0.0533,  0.0481,  0.0443,\n",
      "          0.0585,  0.0309,  0.0393,  0.0303, -0.2309,  0.0342,  0.0486,  0.0360,\n",
      "          0.0469,  0.0474, -0.1525,  0.0285,  0.0341,  0.0299, -0.0017,  0.0466,\n",
      "          0.0519,  0.0290,  0.0243,  0.0575,  0.0246,  0.0207,  0.0495,  0.0390,\n",
      "          0.0560,  0.1244,  0.0636,  0.0661,  0.0302, -0.0294,  0.0484,  0.2052,\n",
      "          0.0365,  0.0297]], device='cuda:0')\n",
      "tensor([-0.0008], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = Net(128)\n",
    "\n",
    "model.load_state_dict(torch.load('../weights/2.pth'))\n",
    "\n",
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
