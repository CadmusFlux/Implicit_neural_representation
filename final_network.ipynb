{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, imageio\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist= datasets.MNIST(root='./../data', train=True,download=False, transform=mnist_trans)\n",
    "mnist = torch.utils.data.random_split(mnist, [10000, len(mnist)-10000])[0] # gets split into two parts [10k and 50k] and we slice 0 index.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.0.dev20230622+cu121  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pixel_coordinates(img):\n",
    "    \n",
    "    height, width, _ = img.permute(1,2,0).shape\n",
    "    \n",
    "    # Generate coordinates along the x-axis and y-axis\n",
    "    x_coords = np.linspace(0, 1, width, endpoint=False)\n",
    "    y_coords = np.linspace(0, 1, height, endpoint=False)\n",
    "    \n",
    "    # Create a meshgrid of coordinates\n",
    "    x_mesh, y_mesh = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "    # Stack the coordinates and reshape to obtain the final output\n",
    "    coordinates = np.stack([x_mesh, y_mesh], axis=-1)\n",
    "    \n",
    "    return coordinates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Feature mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_mapping(x, B):\n",
    "    '''\n",
    "        cos and sin of input are joined together to increased the dimension to 4.\n",
    "    '''\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        B = B.to(x)\n",
    "        x_proj = (2.*np.pi*x) @ B.T #512,2\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)    #  2*len(B) #512,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(coord_input,image,map_dict,mapping = 'none'):\n",
    "\n",
    "    coord_input = input_mapping(coord_input, map_dict[mapping])\n",
    "    \n",
    "    test_data = [coord_input, image]\n",
    "    train_data = [coord_input[::2], image[::2]]\n",
    "    \n",
    "    train_x = torch.tensor(train_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    train_y = torch.tensor(train_data[1])#.reshape(-1,3)\n",
    "    test_x = torch.tensor(test_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    test_y = torch.tensor(test_data[1])#.reshape(-1,3)\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50) # input has 4 dimensions.\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,4) # input has 4 dimensions.\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x,train_y,test_x,test_y = batch_loader(xy_grid,x.permute(1,2,0),B_dict,keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = test_x.to(device)\n",
    "# target = test_y.to(device)\n",
    "\n",
    "# model = Net(input.shape[2]).to(device)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(1501)):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     generated = model(input)\n",
    "\n",
    "#     loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#       print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "#       plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "#       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(coordinates,image,img_no,dictionary,mapping,save = False):\n",
    "\n",
    "    train_x,train_y,test_x,test_y = batch_loader(coordinates,image.permute(1,2,0),dictionary,mapping)\n",
    "\n",
    "    input = test_x.to(device)\n",
    "    target = test_y.to(device)\n",
    "\n",
    "    model = Net(input.shape[2]).to(device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "\n",
    "    for epoch in range(1501):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        generated = model(input)\n",
    "\n",
    "        loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 300 == 0:\n",
    "        #     print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "        #     plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        #     plt.show()\n",
    "\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), '../weights2/{fname}.pth'.format(fname=img_no))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_size = 64\n",
    "\n",
    "B_dict = {}\n",
    "\n",
    "B_gauss = torch.normal(0,1,size=(mapping_size,2))\n",
    "for scale in [10.]:\n",
    "    B_dict[f'gauss_{scale}'] = B_gauss * scale\n",
    "\n",
    "for k in B_dict:\n",
    "    keys = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = mnist[0]\n",
    "\n",
    "xy_grid = torch.from_numpy(generate_pixel_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [mnist[i][1] for i in range(len(mnist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mnist,\"../data/mnist.pt\")\n",
    "\n",
    "with open(\"../data/labels\", \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023993492126464844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331bb994c7314acba2a245f77923984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MLAI\\lib\\site-packages\\torch\\utils\\_device.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm(range(len(mnist))):\n",
    "\n",
    "    model_weights(xy_grid,mnist[index][0],index,B_dict,keys,save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_weights(img_no):\n",
    "\n",
    "  path = 'C:/Users/ayush/OneDrive/Desktop/UoE/Dissertation/Implicit Networks/weights2/{fname}.pth'.format(fname=img_no)\n",
    "\n",
    "  model.load_state_dict(torch.load(path))\n",
    "\n",
    "  weights = []\n",
    "\n",
    "  for param in model.parameters():\n",
    "\n",
    "    weights.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "  return np.array(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(128)\n",
    "weights_dict = {}\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "  weights_dict[str(i)] = flatten_weights(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('C:/Users/ayush/OneDrive/Desktop/UoE/Dissertation/Implicit Networks/data/weights_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9051,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dict['9999'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\Implicit Networks\\Implicit_neural_representation\\final_network.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayush/OneDrive/Desktop/UoE/Dissertation/Implicit%20Networks/Implicit_neural_representation/final_network.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(weights_dict[\u001b[39m'\u001b[39;49m\u001b[39m9999\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mresize((\u001b[39m1\u001b[39;49m,\u001b[39m28\u001b[39;49m,\u001b[39m28\u001b[39;49m)))\n",
      "\u001b[1;31mValueError\u001b[0m: cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False"
     ]
    }
   ],
   "source": [
    "plt.imshow(weights_dict['9999'].resize((1,28,28)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
