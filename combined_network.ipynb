{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, imageio\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.0.dev20230622+cu121  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(dataset_name,d_path,gen):\n",
    "\n",
    "  if dataset_name == \"mnist\":\n",
    "    mnist_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.MNIST(root=os.path.join(d_path,'datasets'), train=True,download=False, transform=mnist_trans)\n",
    "\n",
    "\n",
    "  if dataset_name == \"cifar\":\n",
    "    cifar_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root=os.path.join(d_path,'datasets'), train=True,\n",
    "                                        download=True, transform=cifar_trans)\n",
    "\n",
    "  return torch.utils.data.random_split(dataset, [10000, len(dataset)-10000],generator=gen)[0] # gets split into two parts [10k and 50k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pixel_coordinates(img):\n",
    "\n",
    "    height, width, _ = img.permute(1,2,0).shape\n",
    "\n",
    "    # Generate coordinates along the x-axis and y-axis\n",
    "    x_coords = np.linspace(0, 1, width, endpoint=False)\n",
    "    y_coords = np.linspace(0, 1, height, endpoint=False)\n",
    "\n",
    "    # Create a meshgrid of coordinates\n",
    "    x_mesh, y_mesh = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Stack the coordinates and reshape to obtain the final output\n",
    "    coordinates = np.stack([x_mesh, y_mesh], axis=-1)\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_mapping(x, B):\n",
    "    '''\n",
    "        cos and sin of input are joined together to increased the dimension to 4.\n",
    "    '''\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        B = B.to(x)\n",
    "        x_proj = (2.*np.pi*x) @ B.T #512,2\n",
    "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)    #  2*len(B) #512,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(coord_input,image,map_dict,mapping = 'none'):\n",
    "\n",
    "    coord_input = input_mapping(coord_input, map_dict[mapping])\n",
    "\n",
    "    test_data = [coord_input, image]\n",
    "    train_data = [coord_input[::2], image[::2]]\n",
    "\n",
    "    train_x = torch.tensor(train_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    train_y = torch.tensor(train_data[1])#.reshape(-1,3)\n",
    "    test_x = torch.tensor(test_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    test_y = torch.tensor(test_data[1])#.reshape(-1,3)\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50) # input has 4 dimensions.\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,4) # input has 4 dimensions.\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(coordinates,image,img_no,dictionary,mapping,d_path,save = False):\n",
    "\n",
    "    train_x,train_y,test_x,test_y = batch_loader(coordinates,image.permute(1,2,0),dictionary,mapping)\n",
    "\n",
    "    input = test_x.to(device)\n",
    "    target = test_y.to(device)\n",
    "\n",
    "    model = Net(input.shape[2]).to(device)\n",
    "            \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "    ep = 1501\n",
    "    \n",
    "    for epoch in range(ep):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(input.shape)\n",
    "        generated = model(input)\n",
    "\n",
    "        loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 300 == 0:\n",
    "        #     print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "        #     plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        #     plt.show()\n",
    "    \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), os.path.join(d_path,'weights\\{fname}.pth'.format(fname=img_no)))\n",
    "        # plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        # plt.show()\n",
    "        # print(generated.shape)\n",
    "        save_image(generated.permute(2,0,1), os.path.join(d_path,'reconstructed_images\\{fname}.png'.format(fname=img_no)))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cifar\"\n",
    "generator_rand = torch.Generator(device = device).manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"cifar\":\n",
    "    dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\Implicit Networks\\data\\cifar\"\n",
    "    \n",
    "if dataset_name == \"mnist\":\n",
    "    dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\Implicit Networks\\data\\mnist\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_loader(dataset_name,dataset_path,generator_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_size = 64\n",
    "\n",
    "B_dict = {}\n",
    "\n",
    "B_gauss = torch.normal(0,1,size=(mapping_size,2))\n",
    "for scale in [10.]:\n",
    "    B_dict[f'gauss_{scale}'] = B_gauss * scale\n",
    "\n",
    "for k in B_dict:\n",
    "    keys = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = dataset[0]\n",
    "\n",
    "xy_grid = torch.from_numpy(generate_pixel_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [dataset[i][1] for i in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset,os.path.join(dataset_path,\"datasets\",dataset_name+'_10K_dataset.pt'))\n",
    "\n",
    "# with open(os.path.join(dataset_path,\"datasets\",dataset_name+\"_labels\"), \"wb\") as fp:   #Pickling\n",
    "#   pickle.dump(labels, fp)\n",
    "\n",
    "# for layer in model.children():\n",
    "#     if hasattr(layer, 'reset_parameters'):\n",
    "#         layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024695396423339844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d135b0bf2c94474acaa9b35ceae30d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MLAI\\lib\\site-packages\\torch\\utils\\_device.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm(range(len(dataset))):\n",
    "\n",
    "    model_weights(xy_grid,dataset[index][0],index,B_dict,keys,dataset_path,save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Weights Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_weights(img_no):\n",
    "\n",
    "  path = os.path.join(dataset_path,'weights\\{fname}.pth'.format(fname=img_no))\n",
    "\n",
    "  model.load_state_dict(torch.load(path))\n",
    "\n",
    "  weights = []\n",
    "\n",
    "  for param in model.parameters():\n",
    "\n",
    "    weights.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "  return np.array(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(128)\n",
    "weights_dict = {}\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "  weights_dict[str(i)] = flatten_weights(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_path,'datasets','weights_dictionary.pkl'), 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to plot trained and untrained weights\n",
    "\n",
    "# import copy\n",
    "\n",
    "# sd_ref = copy.deepcopy(model.state_dict()) # --> Add this before and after the training loop with different variable names and return it.\n",
    "\n",
    "# init_weights_list = []\n",
    "# trained_weights_list = []\n",
    "\n",
    "# for param in init_weights.values():\n",
    "#     init_weights_list.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "# for param in trained_weights.values():\n",
    "#     trained_weights_list.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "# plt.imshow(np.array(init_weights_list)[:9025].reshape(95,95),cmap = 'gray')\n",
    "# plt.savefig(r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\Implicit Networks\\data\\cifar\\plots\\Observations\\Cifar_untrained_weights.pdf\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
