{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data_utils\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import utils\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, imageio\n",
    "import pickle\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.1.0.dev20230622+cu121  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(dataset_name,d_path,gen):\n",
    "\n",
    "  if dataset_name == \"mnist\":\n",
    "    mnist_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.MNIST(root=os.path.join(d_path,'datasets'), train=True,download=True, transform=mnist_trans)\n",
    "\n",
    "\n",
    "  if dataset_name == \"cifar\":\n",
    "    cifar_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Normalize(0, 1.0),\n",
    "    transforms.CenterCrop((28))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(root=os.path.join(d_path,'datasets'), train=True,\n",
    "                                        download=True, transform=cifar_trans)\n",
    "\n",
    "  return torch.utils.data.random_split(dataset, [10000, len(dataset)-10000],generator=gen)[0] # gets split into two parts [10k and 50k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pixel_coordinates(img):\n",
    "\n",
    "    height, width, _ = img.permute(1,2,0).shape\n",
    "\n",
    "    # Generate coordinates along the x-axis and y-axis\n",
    "    x_coords = np.linspace(0, 1, width, endpoint=False)\n",
    "    y_coords = np.linspace(0, 1, height, endpoint=False)\n",
    "\n",
    "    # Create a meshgrid of coordinates\n",
    "    x_mesh, y_mesh = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Stack the coordinates and reshape to obtain the final output\n",
    "    coordinates = np.stack([x_mesh, y_mesh], axis=-1)\n",
    "\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_mapping(x, B):\n",
    "#     '''\n",
    "#         cos and sin of input are joined together to increased the dimension to 4.\n",
    "#     '''\n",
    "#     if B is None:\n",
    "#         return x\n",
    "#     else:\n",
    "#         B = B.to(x)\n",
    "#         x_proj = (2.*np.pi*x) @ B.T #512,2\n",
    "#         return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)    #  2*len(B) #512,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loader(coord_input,image):\n",
    "\n",
    "    # coord_input = input_mapping(coord_input, map_dict[mapping])\n",
    "\n",
    "    test_data = [coord_input, image]\n",
    "    train_data = [coord_input[::2], image[::2]]\n",
    "\n",
    "    train_x = torch.tensor(train_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    train_y = torch.tensor(train_data[1])#.reshape(-1,3)\n",
    "    test_x = torch.tensor(test_data[0])#.reshape(-1,coord_input.shape[2]) # because input has 4 dimension\n",
    "    test_y = torch.tensor(test_data[1])#.reshape(-1,3)\n",
    "\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50) # input has 4 dimensions.\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1,4) # input has 4 dimensions.\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(coordinates,image,img_no,d_path,save = False):\n",
    "\n",
    "    train_x,train_y,test_x,test_y = batch_loader(coordinates,image.permute(1,2,0))\n",
    "\n",
    "    input = test_x.to(device)\n",
    "    target = test_y.to(device)\n",
    "    \n",
    "    model = Net(input.shape[2]).to(device)\n",
    "            \n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-2)\n",
    "\n",
    "    ep = 1501\n",
    "    \n",
    "    for epoch in range(ep):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(input.shape)\n",
    "        generated = model(input)\n",
    "        # print(model)\n",
    "        loss = torch.nn.functional.l1_loss(target, generated)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 300 == 0:\n",
    "        #     print('Epoch %d, loss = %.03f' % (epoch, float(loss)))\n",
    "        #     plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        #     plt.show()\n",
    "    \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), os.path.join(d_path,'weights\\{fname}.pth'.format(fname=img_no)))\n",
    "        # plt.imshow(generated.detach().cpu().numpy(),cmap='gray')\n",
    "        # plt.show()\n",
    "        # print(generated.shape)\n",
    "        \n",
    "        \n",
    "        save_image(generated.permute(2,0,1), os.path.join(d_path,'reconstructed_images\\{fname}.png'.format(fname=img_no)))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mnist\"\n",
    "method_type = \"non-fourier\"\n",
    "generator_rand = torch.Generator(device = device).manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"cifar\":\n",
    "    dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\cifar\"\n",
    "    if method_type == \"non-fourier\":\n",
    "        dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\cifar\"\n",
    "    \n",
    "if dataset_name == \"mnist\":\n",
    "    dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\"\n",
    "    if method_type == \"non-fourier\":\n",
    "        dataset_path = r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 15310450.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 23304288.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 9407801.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\non-fourier\\data\\mnist\\datasets\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_loader(dataset_name,dataset_path,generator_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_size = 64\n",
    "\n",
    "# B_dict = {}\n",
    "\n",
    "# B_gauss = torch.normal(0,1,size=(mapping_size,2))\n",
    "# for scale in [10.]:\n",
    "#     B_dict[f'gauss_{scale}'] = B_gauss * scale\n",
    "\n",
    "# for k in B_dict:\n",
    "#     keys = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = dataset[0]\n",
    "\n",
    "xy_grid = torch.from_numpy(generate_pixel_coordinates(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [dataset[i][1] for i in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset,os.path.join(dataset_path,\"datasets\",dataset_name+'_10K_dataset.pt'))\n",
    "\n",
    "with open(os.path.join(dataset_path,\"datasets\",dataset_name+\"_labels\"), \"wb\") as fp:   #Pickling\n",
    "  pickle.dump(labels, fp)\n",
    "\n",
    "# for layer in model.children():\n",
    "#     if hasattr(layer, 'reset_parameters'):\n",
    "#         layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in tqdm(range(2)):\n",
    "#         model_weights(xy_grid,dataset[index][0],index,dataset_path,save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Weights Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_weights(img_no):\n",
    "\n",
    "  path = os.path.join(dataset_path,'weights\\{fname}.pth'.format(fname=img_no))\n",
    "\n",
    "  model.load_state_dict(torch.load(path))\n",
    "\n",
    "  weights = []\n",
    "\n",
    "  for param in model.parameters():\n",
    "\n",
    "    weights.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "  return np.array(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(2)\n",
    "weights_dict = {}\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "  weights_dict[str(i)] = flatten_weights(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_path,'datasets','weights_dictionary.pkl'), 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(128)\n",
    "img_no = 1\n",
    "\n",
    "path = os.path.join(dataset_path,'weights\\{fname}.pth'.format(fname=img_no))\n",
    "\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n",
      "torch.Size([50])\n",
      "torch.Size([50, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.06351685523986816,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905968a283804a90bd5df212aa784b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MLAI\\lib\\site-packages\\torch\\utils\\_device.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "loss_mnist_without_fourier = []\n",
    "\n",
    "for index in tqdm(range(len(dataset))):\n",
    "        loss_mnist_without_fourier.append(model_weights(xy_grid,dataset[index][0],index,dataset_path,save=True).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "with open(dataset_path+\"\\experiments\\cross_entro_loss_10K_images_without_fourier.pkl\",'wb') as f:\n",
    "\n",
    "    pickle.dump(loss_mnist_without_fourier,f)\n",
    "\n",
    "with open(dataset_path+\"\\experiments\\cross_entro_loss_10K_images_without_fourier.pkl\",'rb') as f:\n",
    "\n",
    "    loss_mnist_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0868362774783753"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(loss_mnist_saved[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to plot trained and untrained weights\n",
    "\n",
    "# import copy\n",
    "\n",
    "# sd_ref = copy.deepcopy(model.state_dict()) # --> Add this before and after the training loop with different variable names and return it.\n",
    "\n",
    "# init_weights_list = []\n",
    "# trained_weights_list = []\n",
    "\n",
    "# for param in init_weights.values():\n",
    "#     init_weights_list.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "# for param in trained_weights.values():\n",
    "#     trained_weights_list.extend(list(torch.flatten(param.data).detach().cpu().numpy()))\n",
    "\n",
    "# plt.imshow(np.array(init_weights_list)[:9025].reshape(95,95),cmap = 'gray')\n",
    "# plt.savefig(r\"C:\\Users\\ayush\\OneDrive\\Desktop\\UoE\\Dissertation\\Implicit Networks\\data\\cifar\\plots\\Observations\\Cifar_untrained_weights.pdf\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
